å¯¹RNNå’ŒLSTMçš„éå¸¸å¥½æ¦‚è¿°æ–‡ç« ğŸ‘<a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">é“¾æ¥</a><br>
# RNN

<p>RNNæ€è·¯(å›¾æºè§æ°´å°):</p>
<img src="https://pic2.zhimg.com/v2-2d8b631f3354893f91d5a7dc539f17e3_r.jpg" alt="RNN" width="450" height="250">
<p>biRNNæ€è·¯(æ­£å‘+åå‘)(å›¾æºè§æ°´å°):</p>
<img src="https://i-blog.csdnimg.cn/blog_migrate/a474d4cd3bf27baa594e2742ed06c5dc.png" alt="biRNN" width="400" height="250">
RNNæœ¬è´¨ä¸Šå°±æ˜¯åœ¨è®¡ç®—**å½“å‰è¾“å‡ºçš„æ—¶å€™è€ƒè™‘äº†ä¹‹å‰è¾“å…¥çš„å½±å“**ï¼Œåœ¨ä¸æ—¶é—´ç›¸å…³çš„é—®é¢˜æ¯”è¾ƒæœ‰ç”¨ã€‚

```python
# this is a copy of the code in https://pytorch.org/docs/stable/generated/torch.nn.RNN.html
# Efficient implementation equivalent to the following with bidirectional=False

# torch.nn.RNN(input_size, hidden_size, num_layers=1, nonlinearity='tanh',
#              bias=True, batch_first=False, dropout=0.0, bidirectional=False,
3              device=None, dtype=None)

def forward(x, hx=None):
    if batch_first:
        x = x.transpose(0, 1)          # é»˜è®¤æ˜¯(seq_len, batch_size, _)
    seq_len, batch_size, _ = x.size()
    if hx is None:
        hx = torch.zeros(num_layers, batch_size, hidden_size)
        # hxæ˜¯ç”±rnnå±‚æ•° batch_size hidden_size(å¯ä»¥ç†è§£ä¸ºç»è¿‡rnnåyçš„ç»´æ•°) ç»„æˆçš„ æ¯ä¸€ä¸ªseqç»è¿‡äº†rnnåå°±å˜äº†rnn_layers * hidden*size
    h_t_minus_1 = hx
    h_t = hx
    output = []
    for t in range(seq_len):  # é€ä¸ªæ—¶é—´æ­¥éå† biRNNæˆ‘è®¤ä¸ºå°±æ˜¯seq_len - 1 â†’ 0 è¿™æ ·å†æ¨ä¸€é
      for layer in range(num_layers):  # é€å±‚è®¡ç®—
            h_t[layer] = torch.tanh(
            x[t] @ weight_ih[layer].T  # å½“å‰è¾“å…¥ * è¾“å…¥æƒé‡
            + bias_ih[layer]  # è¾“å…¥åç½®
            + h_t_minus_1[layer] @ weight_hh[layer].T  # ä¸Šä¸€æ—¶é—´æ­¥éšè—çŠ¶æ€ * éšè—å±‚æƒé‡
            + bias_hh[layer]  # éšè—å±‚åç½®
            )
        # è¿™é‡Œæ„Ÿè§‰æœ‰ç‚¹é—®é¢˜ for layerå…¶å®æ²¡æœ‰ç”¨å“‡ æœäº†äº›èµ„æ–™å®ç°çš„åŸºæœ¬éšè—å±‚éƒ½ä¸º1 ä¼°è®¡æ˜¯è¿™ä¸ªåŸå› 
       output.append(h_t[-1]) 
       h_t_minus_1 = h_t
    output = torch.stack(output)
    if batch_first:
        output = output.transpose(0, 1)
    return output, h_t
```

# LSTM
LSTMç›¸å¯¹äºRNNçš„å¥½å¤„å°±æ˜¯å¼•å…¥äº†**é—å¿˜**è¿™ä¸ªæ¦‚å¿µï¼Œå¯ä»¥æœ‰æ•ˆåœ°è§£å†³æ¢¯åº¦çˆ†ç‚¸çš„é—®é¢˜ã€‚<br>
éå¸¸å¥½çš„ä¸€ç¯‡æ–‡ç« ï¼Œä»‹ç»äº†æ¢¯åº¦çˆ†ç‚¸çš„åŸå› ğŸ‘ <a href="https://blog.csdn.net/mary19831/article/details/129570030">é“¾æ¥</a><br>
åŒæ—¶ç¬”è®°æœ€å¼€å§‹çš„é“¾æ¥ä»‹ç»å¾—éå¸¸æ·±å…¥æµ…å‡ºï¼Œå…¬å¼å¯ä»¥å‚è€ƒPyTorchå®˜æ–¹æ–‡æ¡£ç»™å‡ºçš„<a href="https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html">é“¾æ¥</a> <br>

```python
# å‚è€ƒ:https://zhuanlan.zhihu.com/p/451985132

class lstm_origin(nn.Module):
  def __init__(self, input_size, hidden_size):
      super().__init___()
      self.input_size=input_size
      self.hidden_size=hidden_size

      # input
      self.wii=nn.Parameter(torch.Tensor(input_size, hidden_size))
      self.wih=nn.Parameter(torch.Tensor(hidden_size, hidden_size))
      self.bi=nn.Parameter(torch.Tensor(hidden_size)) # åç½®é‡å°±æŠŠxiçš„éƒ¨åˆ†å’Œhiçš„éƒ¨åˆ†åˆåœ¨ä¸€èµ·äº†

      #output
      self.wio=nn.Parameter(torch.Tensor(input_size, hidden_size))
      self.who=nn.Parameter(torch.Tensor(hidden_size, hidden_size))
      self.bo=nn.Parameter(torch.Tensor(hidden_size))

      #cell
      self.wic=nn.Patameter(torch.Tensor(input_size, hidden_size))
      self.whc=nn.Parameter(torch.Tensor(hidden_size, hidden_size))
      self.bc=nn.Parameter(torch.Tensor(hidden_size))

      #forget
      self.wif=nn.Patameter(torch.Tensor(input_size, hidden_size))
      self.whf=nn.Parameter(torch.Tensor(hidden_size, hidden_size))
      self.bf=nn.Parameter(torch.Tensor(hidden_size))


      self.init_weights()

  def forward(self, x):
      bs, seq_sz, _=x.size()  # é»˜è®¤batch_sizeæ˜¯ç¬¬0ç»´
      hidden_seq=[]
      h_t, c_t=(
              torch.zeros(bs,self.hidden_size).to(x.device),
              torch.zeros(bs,self.hidden_size).to(x.device),
            )
      for t in range(seq_sz):
          x_t=x[:, t, :]   # åªå–ç¬¬tæ—¶é—´æ­¥

          i_t=torch.sigmoid(x_t@self.wii + h_t@self.wih + self.bi)
          f_t=torch.sigmoid(x_t@self.wfi + h_t@self.wfh + self.bf)
          cp_t=torch.tanh(x_t@self.wci + h_t@self.wch + self.bc)
          o_t=torch.sigmoid(x_t@self.woi + h_t@self.woh + self.bo)
          c_t=f_t * c_t + i_t * cp_t
          h_t=o_t * torch.tanh(c_t)   #h_tä¹Ÿå¯ä»¥æ˜¯å®é™…çš„o_t
          # ç°åœ¨çš„sizeæ˜¯(batch_size, hidden_sz) è¿™é‡Œåœ¨æœ€å‰é¢åŠ äº†ä¸€ç»´
          hidden_seq.append(h_t.unsqueeze(0))
      hidden_seq=torch.cat(hidden_seq, dim=0) #å°†æ¯ä¸ªseqçš„è¾“å‡ºæŒ‰dim=0æ‹¼æ¥èµ·æ¥
      hidden_seq=hidden_seq.transpose(0, 1).contiguous() #åˆè½¬å˜æˆ(batch_sz, seq_sz, hidden_sz)
      return hidden_seq, (h_t, c_t)
          

    
      
```
